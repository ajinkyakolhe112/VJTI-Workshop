{"cells":[{"cell_type":"markdown","metadata":{},"source":["# README\n","1. Code is writing in the simplest form possible, to make learning easy. \n","2. In just `5 Code Cells`, you will have done your Kaggle Submission for \"Digit Recognizer\" competition \n","2. Dataset is downloaded from `Huggingface Datasets`, Model is written in Pytorch, Training Loop is written in pytorch"]},{"cell_type":"markdown","metadata":{},"source":["# 1. Simple Dataset downloading Pipeline"]},{"cell_type":"code","execution_count":1,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-06-07T13:31:00.182407Z","iopub.status.busy":"2024-06-07T13:31:00.181391Z","iopub.status.idle":"2024-06-07T13:32:24.343151Z","shell.execute_reply":"2024-06-07T13:32:24.341579Z","shell.execute_reply.started":"2024-06-07T13:31:00.182355Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c7a9254c89a443388ee9facc1b7a24c2","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/60000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0f1234c0c1734dbaa7ca3b3bb7ba31b4","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import torch, torch.nn as nn\n","import torchvision, torchinfo, torchmetrics\n","import datasets as huggingface_datasets\n","from tqdm import tqdm\n","\n","device        = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","BATCH_SIZE    = 5\n","# IMAGE_RESIZE  = 28,28\n","\n","def DOWNLOAD_DATASETS():\n","    # Download\n","    dataset            = huggingface_datasets.load_dataset(\"mnist\", ) # streaming = True)\n","    training_dataset   = dataset['train']\n","    validation_dataset = dataset['test']\n","\n","    # Transform\n","    transformations_group = torchvision.transforms.Compose([\n","        torchvision.transforms.ToTensor(), # Converts every pixel into value between 0 & 1. \n","        # torchvision.transforms.Resize(size=config.IMAGE_RESIZE)\n","    ])\n","\n","    def transform_datasets(examples):\n","        examples[\"image_tensors\"] = []\n","\n","        for image in examples['image']:\n","            transformed_image = transformations_group(image)\n","            examples['image_tensors'].append(transformed_image)\n","\n","        return examples\n","\n","    training_dataset       = training_dataset   .map(transform_datasets  , batched= True)\n","    validation_dataset     = validation_dataset .map(transform_datasets  , batched= True)\n","\n","    # Convert\n","    new_training_dataset   = training_dataset   .with_format(\"torch\", columns=['label', 'image_tensors'], dtype = torch.float32)\n","    new_validation_dataset = validation_dataset .with_format(\"torch\", columns=['label', 'image_tensors'], dtype = torch.float32)\n","\n","    TOTAL_BATCHES = len(training_dataset) / BATCH_SIZE\n","    \n","    training_dataloader   = torch.utils.data.DataLoader( dataset= new_training_dataset   , batch_size= BATCH_SIZE, shuffle= True )\n","    validation_dataloader = torch.utils.data.DataLoader( dataset= new_validation_dataset , batch_size= BATCH_SIZE, shuffle= True )\n","    \n","    return training_dataset, validation_dataset, training_dataloader, validation_dataloader\n","\n","training_dataset, validation_dataset, training_dataloader, validation_dataloader = DOWNLOAD_DATASETS();\n","assert next(iter(training_dataloader)) is not None\n","assert next(iter(validation_dataloader)) is not None"]},{"cell_type":"markdown","metadata":{},"source":["# 2. Simple Model Training Pipeline"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-06-07T13:33:30.915601Z","iopub.status.busy":"2024-06-07T13:33:30.915058Z","iopub.status.idle":"2024-06-07T13:33:30.937479Z","shell.execute_reply":"2024-06-07T13:33:30.935621Z","shell.execute_reply.started":"2024-06-07T13:33:30.915561Z"},"trusted":true},"outputs":[],"source":["lr      = 0.001 # learning_rate\n","epochs  = 10 # How much to train a model\n","device  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","def TRAIN_MODEL(model, training_dataloader, validation_dataloader):\n","\n","    model.train(mode=True)\n","    OPTIMIZER = torch.optim.SGD ( params= model.parameters(), lr= lr ) # Using torch.optimizer algorithm\n","    metric    = torchmetrics.Accuracy(task=\"multiclass\", num_classes= 10 ).to(device)\n","    \n","    for epoch_no in range(epochs):        \n","        for batch_no, batch_dictionary in enumerate(progress_bar := tqdm(training_dataloader)):\n","            x_actual = batch_dictionary['image_tensors'].to(device)\n","            y_actual = batch_dictionary['label'].to(device)\n","\n","            y_predicted_LOGITS = model.forward               (x_actual)\n","            y_predicted_probs  = nn.functional.softmax       (y_predicted_LOGITS, dim= 1)\n","            loss               = nn.functional.cross_entropy (y_predicted_LOGITS, y_actual.to(torch.int64))\n","            \n","            OPTIMIZER.zero_grad()\n","            loss.backward()\n","            # dError_dParameters    = torch.autograd.grad( outputs = ERROR_FUNC( y_predicted, y_actual ), inputs = model.parameters())\n","            # Parameters of layer 1 are not dependent on any other parameters\n","            # Parameters of layer 2 are dependent on layer 1 parameters\n","            # Parameters of layer 3 are dependent on layer 2 parameters which are dependent on layer 1 parameters\n","            # Finding complicated rate of change of such nested parameters is done automatically when we do loss.backward()\n","            OPTIMIZER.step()\n","            \"\"\"\n","            for (name, weight), gradient in zip(model.named_parameters(), dError_dWeights):\n","                weight = weight - gradient * LEARNING_RATE\n","                print(f\"Parameters of layer: {name} have these many {torch.count_nonzero(gradient)} updates out of {torch.count(gradient)})\n","            \"\"\"\n","\n","            loss_batch      = loss.item()\n","            accuracy_batch  = metric(y_predicted_LOGITS, y_actual)\n","            training_accuracy_avg_epoch = metric.compute() # calculates average accuracy across epoch automatically\n","\n","            metrics_per_batch = {\n","                \"loss_batch\": loss_batch,\n","                \"accuracy_running_average\": training_accuracy_avg_epoch,\n","            }\n","            progress_bar.set_description(f'batch_no = {batch_no},\\t loss_batch = {loss_batch:0.4f},\\t accuracy_avg = {training_accuracy_avg_epoch:0.4f}')\n","\n","        metric.reset()\n","        \n","        loss_validation, accuracy_validation = EVALUATE_MODEL(model, validation_dataloader)\n","        print(f'epoch_no = {epoch_no}, training_loss = {loss_batch:0.4f}, validation_loss = {loss_validation:0.4f},\\t training_accuracy = {accuracy_batch:0.4f}, validation_accuracy = {accuracy_validation:0.4f}')\n","        model.train(mode=False)\n","\n","def EVALUATE_MODEL(model, validation_dataloader):\n","    model.eval()\n","    metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(device)\n","    with torch.no_grad():\n","        for batch_no, batch_dictionary in enumerate(validation_dataloader):\n","            x_actual = batch_dictionary['image_tensors'].to(device)\n","            y_actual = batch_dictionary['label'].to(device)\n","\n","            y_predicted_LOGITS = model.forward                 (x_actual)\n","            loss               = nn.functional.cross_entropy   (y_predicted_LOGITS, y_actual.to(torch.int64)).item()\n","            accuracy_batch     = metric                        (y_predicted_LOGITS, y_actual).item()\n","\n","        testing_accuracy_avg = metric.compute().item()\n","        return loss, testing_accuracy_avg"]},{"cell_type":"markdown","metadata":{},"source":["# 3. Simple Model Architecture"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-07T13:33:32.828246Z","iopub.status.busy":"2024-06-07T13:33:32.827615Z","iopub.status.idle":"2024-06-07T13:33:32.847116Z","shell.execute_reply":"2024-06-07T13:33:32.845325Z","shell.execute_reply.started":"2024-06-07T13:33:32.828173Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","Sequential                               [1, 10]                   --\n","├─Flatten: 1-1                           [1, 784]                  --\n","├─Linear: 1-2                            [1, 40]                   31,400\n","│    └─weight                                                      ├─31,360\n","│    └─bias                                                        └─40\n","├─ReLU: 1-3                              [1, 40]                   --\n","├─Linear: 1-4                            [1, 30]                   1,230\n","│    └─weight                                                      ├─1,200\n","│    └─bias                                                        └─30\n","├─ReLU: 1-5                              [1, 30]                   --\n","├─Linear: 1-6                            [1, 10]                   310\n","│    └─weight                                                      ├─300\n","│    └─bias                                                        └─10\n","==========================================================================================\n","Total params: 32,940\n","Trainable params: 32,940\n","Non-trainable params: 0\n","Total mult-adds (Units.MEGABYTES): 0.03\n","==========================================================================================\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.00\n","Params size (MB): 0.13\n","Estimated Total Size (MB): 0.14\n","==========================================================================================\n"]}],"source":["model_random_parameters = torch.nn.Sequential(\n","    \n","    torch.nn.Flatten(start_dim=1),         # Dim:BCHW -> (0:B , 1:C, 2:H, 3:W)\n","\n","    torch.nn.Linear(in_features = 28*28*1  , out_features   = 40   ), torch.nn.ReLU(),                 # LAYER 1: 1st Hidden Layer\n","    torch.nn.Linear(in_features = 40       , out_features = 30   ), torch.nn.ReLU(),                 # LAYER 2: 2nd Hidden Layer\n","\n","    torch.nn.Linear(in_features = 30       , out_features = 10   ),                                  # OUTPUT LAYER\n",")\n","\n","model = model_random_parameters\n","model = model.to(device)                    # Model Size / Number of Parameters are important\n","\n","torchinfo.summary(model, input_size= (1,1*28*28), verbose=2);"]},{"cell_type":"markdown","metadata":{},"source":["## Details of Problem Complexity\n","- 0.00784 Mega Pixel Image\n","- 60,000 of such images\n","- They are black and white\n","- They contain only numbers 0 to 9\n","- Even this SIMPLE PROBLEM, requires a MODEL of MINIMUM 10,000 parameters"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-06-07T13:33:38.911261Z","iopub.status.busy":"2024-06-07T13:33:38.910791Z","iopub.status.idle":"2024-06-07T13:34:21.173024Z","shell.execute_reply":"2024-06-07T13:34:21.170955Z","shell.execute_reply.started":"2024-06-07T13:33:38.911224Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["batch_no = 2191,\t loss_batch = 1.9750,\t accuracy_avg = 0.3898:  18%|█▊        | 2176/12000 [24:46<00:46, 213.00it/s] "]}],"source":["TRAIN_MODEL (model, training_dataloader, validation_dataloader)"]},{"cell_type":"markdown","metadata":{},"source":["# 4. Kaggle Competition Submission"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-21T06:12:45.870812Z","iopub.status.busy":"2024-05-21T06:12:45.869974Z","iopub.status.idle":"2024-05-21T06:12:54.920105Z","shell.execute_reply":"2024-05-21T06:12:54.919254Z","shell.execute_reply.started":"2024-05-21T06:12:45.870780Z"},"trusted":true},"outputs":[],"source":["!find /kaggle/input\n","\n","import pandas as pd\n","import torch, torchvision\n","submission_test = pd.read_csv(\"/kaggle/input/digit-recognizer/test.csv\")\n","\n","x = torch.tensor(submission_test.values.reshape(submission_test.shape[0], 1, 28, 28), dtype=torch.float32)\n","x = x.to(device)\n","\n","def tensor_to_images(x):\n","    images = {}\n","    for index in range(x.shape[0]):\n","        images[str(index)] = torchvision.transforms.ToPILImage(mode = \"L\" )(x[index])\n","    return images\n","\n","images = tensor_to_images(x)\n","\n","# TODO: Figure Out how to do Transforms same way as training Data\n","transformations_list = torchvision.transforms.Compose([\n","    torchvision.transforms.ToPILImage(), # Because Data is not saved as Image, we need to first convert it in Image and then convert back to Tensor\n","    \n","    torchvision.transforms.ToTensor(),\n","])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-20T06:18:04.925334Z","iopub.status.busy":"2024-05-20T06:18:04.924343Z","iopub.status.idle":"2024-05-20T06:18:04.972198Z","shell.execute_reply":"2024-05-20T06:18:04.971241Z","shell.execute_reply.started":"2024-05-20T06:18:04.925293Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","y_prediction_logits = model(x)\n","y_labels_predicted  = torch.argmax(y_prediction_logits, dim = 1)\n","\n","submission          = pd.DataFrame({'ImageId' : torch.arange(1, len(y_labels_predicted) + 1).cpu(), \n","                                    'Label'   : y_labels_predicted.cpu()})\n","submission.to_csv('submission.csv', index=False)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":861823,"sourceId":3004,"sourceType":"competition"}],"dockerImageVersionId":30698,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":4}
